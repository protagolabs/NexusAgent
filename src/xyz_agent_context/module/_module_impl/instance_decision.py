"""
@file_name: instance_decision.py
@author: NetMind.AI
@date: 2025-12-22
@description: Instance Intelligent Decision Module

Provides LLM-driven Module Instance intelligent decision functionality:
1. llm_decide_instances - Uses LLM to decide which Module Instances are needed
2. dict_to_module_instance - Converts dictionary to ModuleInstance object

Uses OpenAI Agents SDK to ensure correct LLM output format.
"""

from typing import List, Optional
from pydantic import BaseModel, Field
from loguru import logger

from xyz_agent_context.utils import utc_now

from xyz_agent_context.schema.module_schema import ModuleInstance, InstanceStatus
from xyz_agent_context.schema.decision_schema import DirectTriggerConfig
from xyz_agent_context.agent_framework.openai_agents_sdk import OpenAIAgentsSDK
from xyz_agent_context.module._module_impl.prompts import INSTANCE_DECISION_PROMPT_TEMPLATE


# ===== LLM Output Schema Definition =====
# Using Pydantic to define strict JSON Schema, ensuring correct LLM output format

class JobConfig(BaseModel):
    """
    JobModule specific configuration (LLM output format)

    When module_class is "JobModule", LLM needs to fill in this configuration.

    Job Types and Trigger Configuration:
    1. ONE_OFF (one-time task):
       - Use scheduled_at: ISO 8601 format, e.g., "2025-01-01T09:00:00"
       - Or leave both null for dependency trigger (wait for prerequisite task)

    2. SCHEDULED (periodic task):
       - Use cron: cron expression, e.g., "0 9 * * *" means every day at 9am
       - Or use interval_seconds without end_condition for simple interval

    3. ONGOING (continuous task, e.g., sales follow-up):
       - Use interval_seconds + end_condition (REQUIRED for ONGOING!)
       - Optional max_iterations to limit execution count
       - Example: Sales follow-up until customer buys or refuses

    Context parameters (Feature 2.2 + 3.1):
    - related_entity_id: Target user ID for this job (execution identity)
      When job executes, this ID will be used as the main user_id
    """
    title: str = Field(..., description="Task title")
    scheduled_at: Optional[str] = Field(
        default=None,
        description="Execution time for one-time task (ISO 8601 format, e.g., '2025-01-01T09:00:00'). Use cron field for periodic tasks"
    )
    cron: Optional[str] = Field(
        default=None,
        description="Cron expression for periodic task (e.g., '0 9 * * *' means every day at 9am, '0 0 * * 1' means every Monday at midnight)"
    )
    # ONGOING type required fields
    interval_seconds: Optional[int] = Field(
        default=None,
        description="Interval in seconds between checks (REQUIRED for ONGOING type). E.g., 86400 = check once per day"
    )
    end_condition: Optional[str] = Field(
        default=None,
        description="Termination condition for ONGOING jobs (REQUIRED for ONGOING type). Natural language description. E.g., 'Customer explicitly buys or explicitly refuses'"
    )
    max_iterations: Optional[int] = Field(
        default=None,
        description="Maximum execution count for ONGOING jobs. Job auto-completes when reached. Optional safety limit."
    )
    priority: int = Field(default=5, description="Priority (1-10, default 5)")
    payload: str = Field(default="", description="Execution instructions (natural language)")
    related_entity_id: Optional[str] = Field(
        default=None,
        description=(
            "Target user ID for this job (REQUIRED for most jobs). Rules: "
            "(1) If the job is for Agent to work and report back to the requester, put the requester's user_id; "
            "(2) If the job involves another user (e.g., sales to a customer), put that target user's user_id. "
            "This user_id will be used as the main identity when job executes (loading their context, Narrative, etc)."
        )
    )


class InstanceDict(BaseModel):
    """
    Dictionary representation of a single Instance (LLM output format)

    Module Categories:
    - Capability Modules: ChatModule, AwarenessModule, GeminiRAGModule, SocialNetworkModule, BasicInfoModule
      These are Agent capabilities, do NOT set depends_on
    - Task Modules: JobModule
      Represents a task to be executed, only JobModule can set depends_on

    Important Field Descriptions:
    - task_key: Semantic identifier
      - Capability type: Use module name, e.g., "chat", "awareness", "rag"
      - Task type: Use task description, e.g., "competitor_research", "write_report"
    - depends_on: JobModule only, indicates waiting for prerequisite Job to complete
    - job_config: JobModule specific configuration

    Notes:
    - instance_id is auto-generated by code based on task_key, LLM can leave it empty
    - dependencies is auto-converted by code based on depends_on, LLM doesn't need to fill
    """
    # ===== Semantic Identifier (LLM fills) =====
    task_key: str = Field(
        default="",
        description="Semantic identifier. Capability type uses module name like 'chat'; Task type uses task description like 'competitor_research'"
    )
    depends_on: List[str] = Field(
        default_factory=list,
        description="[JobModule ONLY] List of dependent Job task_keys. Do NOT set this for Capability Modules"
    )

    # ===== Basic Fields =====
    instance_id: str = Field(default="", description="Instance unique identifier (auto-generated by code, LLM can leave empty)")
    module_class: str = Field(..., description="Module class name")
    description: str = Field(default="", description="Role description")
    status: str = Field(default="active", description="Status: active, in_progress, blocked, completed, failed")
    dependencies: List[str] = Field(default_factory=list, description="List of dependent instance_ids (auto-converted by code)")

    # ===== JobModule Specific Configuration =====
    job_config: Optional[JobConfig] = Field(
        default=None,
        description="JobModule specific configuration, required when module_class is 'JobModule'"
    )


class InstanceDecisionOutput(BaseModel):
    """
    LLM Instance Decision Output Structure

    This is the JSON structure that LLM needs to output, using Structured Outputs to ensure correct format.
    """
    reasoning: str = Field(
        default="",
        description="Decision reasoning"
    )
    execution_path: str = Field(
        ...,
        description="Execution path: 'agent_loop' or 'direct_trigger'"
    )
    active_instances: List[InstanceDict] = Field(
        default_factory=list,
        description="List of active Instances"
    )
    changes_explanation: str = Field(
        default="{}",
        description="JSON string explaining changes"
    )
    direct_trigger: Optional[DirectTriggerConfig] = Field(
        default=None,
        description="Configuration for DIRECT_TRIGGER mode"
    )
    relationship_graph: str = Field(
        default="",
        description="Module relationship graph (Mermaid graph TD format)"
    )


async def llm_decide_instances(
    user_input: str,
    agent_id: str,
    current_instances: List[ModuleInstance],
    narrative_summary: str = "",
    markdown_history: str = "",
    awareness: str = "",
    capability_modules: Optional[List[dict]] = None,
    current_user_id: str = "",
    job_info_map: Optional[dict] = None
) -> InstanceDecisionOutput:
    """
    Use LLM to decide which Task Module Instances are needed

    Note: Capability Modules (e.g., ChatModule, AwarenessModule, etc.) are automatically loaded via rules;
    LLM only needs to decide on the creation and management of Task Modules (e.g., JobModule).

    Args:
        user_input: User input content
        agent_id: Agent ID
        current_instances: List of currently active Task Module Instances (task modules only)
        narrative_summary: Narrative summary
        markdown_history: Markdown history
        awareness: Agent awareness content
        capability_modules: List of already loaded capability modules info
        current_user_id: Current user ID (who is chatting now)
        job_info_map: Dict mapping instance_id -> job info (for showing related_entity_id)

    Returns:
        InstanceDecisionOutput: LLM decision output
    """
    # Build prompt
    prompt = _build_decision_prompt(
        user_input=user_input,
        current_instances=current_instances,
        narrative_summary=narrative_summary,
        markdown_history=markdown_history,
        awareness=awareness,
        capability_modules=capability_modules or [],
        current_user_id=current_user_id,
        job_info_map=job_info_map or {}
    )

    logger.info("Instance Decision: Calling LLM for decision")
    logger.debug(f"Prompt length: {len(prompt)} characters")

    try:
        # Use OpenAIAgentsSDK's llm_function
        sdk = OpenAIAgentsSDK()
        result = await sdk.llm_function(
            instructions="You are an intelligent Module Instance manager. Make decisions based on user input.",
            user_input=prompt,
            output_type=InstanceDecisionOutput,
        )

        # result is RunResult, get the parsed Pydantic object via .final_output
        output: InstanceDecisionOutput = result.final_output
        logger.success(f"Instance Decision: LLM decision complete, path={output.execution_path}")
        return output

    except Exception as e:
        logger.error(f"Instance Decision: LLM call failed: {e}")
        # Return default decision (keep current instances, use AGENT_LOOP)
        return _get_default_decision(current_instances)


def dict_to_module_instance(inst_dict: InstanceDict, agent_id: str) -> ModuleInstance:
    """
    Convert InstanceDict to ModuleInstance object

    Args:
        inst_dict: Instance dictionary (from LLM output)
        agent_id: Agent ID

    Returns:
        ModuleInstance object
    """
    # Parse status
    status_map = {
        "active": InstanceStatus.ACTIVE,
        "in_progress": InstanceStatus.IN_PROGRESS,
        "blocked": InstanceStatus.BLOCKED,
        "completed": InstanceStatus.COMPLETED,
        "failed": InstanceStatus.FAILED,
    }
    status = status_map.get(inst_dict.status.lower(), InstanceStatus.ACTIVE)

    return ModuleInstance(
        instance_id=inst_dict.instance_id,
        module_class=inst_dict.module_class,
        description=inst_dict.description,
        status=status,
        agent_id=agent_id,
        dependencies=inst_dict.dependencies,
        created_at=utc_now(),
        last_used_at=utc_now(),
    )


def _build_decision_prompt(
    user_input: str,
    current_instances: List[ModuleInstance],
    narrative_summary: str = "",
    markdown_history: str = "",
    awareness: str = "",
    capability_modules: Optional[List[dict]] = None,
    current_user_id: str = "",
    job_info_map: Optional[dict] = None
) -> str:
    """
    Build the Prompt for LLM decision

    Args:
        user_input: User input
        current_instances: Current active Task Module instances (task modules only)
        narrative_summary: Narrative summary
        markdown_history: History
        awareness: Agent awareness content
        capability_modules: Already loaded capability modules info
        current_user_id: Current user ID (who is chatting now)
        job_info_map: Dict mapping instance_id -> job info (for showing related_entity_id)

    Returns:
        Prompt text
    """
    job_info_map = job_info_map or {}
    # ===== Format already loaded Capability Modules =====
    capability_text = ""
    if capability_modules:
        capability_text = "## Already Loaded Capability Modules (Auto-loaded, no decision needed)\n"
        capability_text += "These capabilities are automatically available to the Agent:\n\n"
        for cap in capability_modules:
            capability_text += f"- **{cap['module_class']}** ({cap['instance_id']})\n"
            capability_text += f"  {cap['description']}\n\n"
    else:
        capability_text = "## Already Loaded Capability Modules\n(None)\n"

    # ===== Format current Task Module Instances =====
    current_instances_text = ""
    if current_instances:
        current_instances_text = "## Current Task Module Instances (Need your decision)\n"
        current_instances_text += "⚠️ **IMPORTANT**: These Jobs ALREADY EXIST. Do NOT create new Jobs with similar titles!\n\n"
        for inst in current_instances:
            status_value = inst.status.value if hasattr(inst.status, 'value') else inst.status
            current_instances_text += f"- **{inst.instance_id}** ({inst.module_class})\n"
            current_instances_text += f"  Description: {inst.description}\n"
            current_instances_text += f"  Status: {status_value}\n"
            if inst.dependencies:
                current_instances_text += f"  Dependencies: {', '.join(inst.dependencies)}\n"
            # Show JobModule's related_entity_id (target user) and title
            if inst.module_class == "JobModule" and inst.instance_id in job_info_map:
                job_info = job_info_map[inst.instance_id]
                target_user = job_info.get("related_entity_id", "N/A")
                job_type = job_info.get("job_type", "N/A")
                job_title = job_info.get("title", "N/A")
                current_instances_text += f"  **Job Title: {job_title}**\n"
                current_instances_text += f"  Target User: {target_user}\n"
                current_instances_text += f"  Job Type: {job_type}\n"
            current_instances_text += "\n"
    else:
        current_instances_text = "## Current Task Module Instances\n(None - no background tasks running)\n"

    # Add history context
    history_context = ""
    if markdown_history:
        # Truncate to recent history (avoid prompt being too long)
        max_history_len = 2000
        if len(markdown_history) > max_history_len:
            history_context = f"\n## History Context (Recent)\n{markdown_history[-max_history_len:]}\n"
        else:
            history_context = f"\n## History Context\n{markdown_history}\n"

    # Add Agent Awareness context
    awareness_context = ""
    if awareness:
        awareness_context = f"""
## Agent Awareness
{awareness}

Note: The Agent's role and characteristics may influence module selection decisions. Please consider the Agent's positioning when making decisions.
"""

    # Build current user information
    current_user_text = ""
    if current_user_id:
        current_user_text = f"""
## Current User
**User ID**: `{current_user_id}`

This is the user who is currently chatting with the Agent. Use this to determine which Jobs are relevant.
"""

    prompt = INSTANCE_DECISION_PROMPT_TEMPLATE.format(
        narrative_summary=narrative_summary if narrative_summary else "New Narrative",
        current_user_text=current_user_text,
        capability_text=capability_text,
        current_instances_text=current_instances_text,
        history_context=history_context,
        awareness_context=awareness_context,
        user_input=user_input,
        current_instances_count=len(current_instances) if current_instances else 0,
    )
    return prompt


def _get_default_decision(current_instances: List[ModuleInstance]) -> InstanceDecisionOutput:
    """
    Get default decision (used when LLM call fails)

    Note: Capability Modules (ChatModule, etc.) are automatically loaded via rules;
    this only handles the default decision for Task Modules (e.g., JobModule).

    Args:
        current_instances: Current Task Module instances

    Returns:
        Default InstanceDecisionOutput
    """
    # Convert current Task Module instances to InstanceDict format
    active_instances = []
    for inst in current_instances:
        status_value = inst.status.value if hasattr(inst.status, 'value') else str(inst.status)
        active_instances.append(InstanceDict(
            instance_id=inst.instance_id,
            module_class=inst.module_class,
            description=inst.description,
            status=status_value,
            dependencies=inst.dependencies
        ))

    # If no Task Module instances, return empty list
    # Capability Modules will be automatically added via rules in loader.py

    return InstanceDecisionOutput(
        execution_path="agent_loop",
        reasoning="Default decision: LLM call failed, keeping current task instances",
        active_instances=active_instances,
        changes_explanation="{}",
        direct_trigger=None,
        relationship_graph=""
    )
